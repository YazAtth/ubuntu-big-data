{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FiM0qfFRgD6"
   },
   "source": [
    "# Lab 3 - Hadoop\n",
    "\n",
    "Hadoop is an open-source framework that allows for the distributed storage and processing of Big Data! It works in three modes:\n",
    "\n",
    "1. Standalone\n",
    "2. Pseudo-distributed\n",
    "3. Fully-distributed\n",
    "\n",
    "To save us a bit of time with setup and system wiring, we'll use the pseudo-distributed mode in this lab. Each of the daemons responsible for distributed storage and processing will run as separate processes on a single host, localhost.\n",
    "\n",
    "Main deamons:\n",
    "* NameNode\n",
    "* Resource Manager\n",
    "* Standby NameNode\n",
    "\n",
    "Worker deamons:\n",
    "* DataNode\n",
    "* Node Manager\n",
    "\n",
    "We'll still need to do a little bit of configuration, but this should be a nice solution using just our one machine! If you'd like a more verbose version of this process and/or a breakdown of the standalone mode, [this source](https://github.com/LMAPcoder/Hadoop-on-Colab) provides a nicely detailed tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2hvCJaoXNFj"
   },
   "source": [
    "## Section 1 - Installing SSH\n",
    "\n",
    "SSH is a cryptographic network protocol for operating securely over an unsecured network. You'll likely have used this before for remotely logging onto the csserver! If you would like a quick overview of SSH and how it works, you can take a look [here](https://www.youtube.com/watch?v=Atbl7D_yPug&t=69s).\n",
    "\n",
    "Hadoop uses passphrase SSH for communication between nodes, so we'll use SSH to define a way for the main node to remotely access every node in our cluster. The best way to do this, is to set up passwordless login for the Hadoop user by generating a public-private key pair. We'll run through this step by step below.\n",
    "\n",
    "**Note:** You may get a warning when you run this cell - you can ignore this. We'll just be using SSH for communication between our nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18293,
     "status": "ok",
     "timestamp": 1706711881924,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "bChIaQp4RgfB",
    "outputId": "0b29afe4-d86f-457d-c61b-bbdd74b3f6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://ports.ubuntu.com/ubuntu-ports jammy InRelease [270 kB]\n",
      "Get:2 http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease [119 kB]\n",
      "Get:3 http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease [109 kB]\n",
      "Get:4 http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease [110 kB]\n",
      "Get:5 http://ports.ubuntu.com/ubuntu-ports jammy/universe arm64 Packages [17.2 MB]\n",
      "Get:6 http://ports.ubuntu.com/ubuntu-ports jammy/multiverse arm64 Packages [224 kB]\n",
      "Get:7 http://ports.ubuntu.com/ubuntu-ports jammy/restricted arm64 Packages [24.2 kB]\n",
      "Get:8 http://ports.ubuntu.com/ubuntu-ports jammy/main arm64 Packages [1758 kB]\n",
      "Get:9 http://ports.ubuntu.com/ubuntu-ports jammy-updates/universe arm64 Packages [1265 kB]\n",
      "Get:10 http://ports.ubuntu.com/ubuntu-ports jammy-updates/main arm64 Packages [1494 kB]\n",
      "Get:11 http://ports.ubuntu.com/ubuntu-ports jammy-updates/restricted arm64 Packages [1231 kB]\n",
      "Get:12 http://ports.ubuntu.com/ubuntu-ports jammy-updates/multiverse arm64 Packages [28.4 kB]\n",
      "Get:13 http://ports.ubuntu.com/ubuntu-ports jammy-backports/universe arm64 Packages [26.2 kB]\n",
      "Get:14 http://ports.ubuntu.com/ubuntu-ports jammy-backports/main arm64 Packages [49.9 kB]\n",
      "Get:15 http://ports.ubuntu.com/ubuntu-ports jammy-security/universe arm64 Packages [996 kB]\n",
      "Get:16 http://ports.ubuntu.com/ubuntu-ports jammy-security/restricted arm64 Packages [1212 kB]\n",
      "Get:17 http://ports.ubuntu.com/ubuntu-ports jammy-security/multiverse arm64 Packages [24.0 kB]\n",
      "Get:18 http://ports.ubuntu.com/ubuntu-ports jammy-security/main arm64 Packages [1223 kB]\n",
      "Fetched 27.4 MB in 3s (8582 kB/s)                           \n",
      "Reading package lists... Done\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# Installing openssh-server\n",
    "!apt-get update\n",
    "!apt-get install openssh-server -qq > /dev/null\n",
    "\n",
    "# Starting our server\n",
    "!service ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1706711883107,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "-KAtUQJjR8f-",
    "outputId": "41cc6cb6-1bd0-4131-d04d-9dfe1c3b4a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private rsa key pair.\n",
      "Created directory '/root/.ssh'.\n",
      "Your identification has been saved in /root/.ssh/id_rsa\n",
      "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
      "The key fingerprint is:\n",
      "SHA256:y8NXvvSQw77hZGQOPuC6jgIducCdJ3d2ReyHtTiNZfw root@56cc797895e3\n",
      "The key's randomart image is:\n",
      "+---[RSA 3072]----+\n",
      "|          o..    |\n",
      "|           o =   |\n",
      "|. ...     o O o  |\n",
      "|..o+ o o . * + E |\n",
      "| o o+ o S . *    |\n",
      "|. o    + + O .   |\n",
      "| .      * + %    |\n",
      "|  .  . . o B *   |\n",
      "|   ...+.    =..  |\n",
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "# Creating a new rsa key pair with empty password\n",
    "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706711883108,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "zYITXjZ0SG5J"
   },
   "outputs": [],
   "source": [
    "# Copying the public key we just generated to authorised keys\n",
    "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
    "\n",
    "# Changing the permissions on the key\n",
    "# Hint: Check \"man chmod\" for information on this command and remember, changing permissions\n",
    "# so be careful with this command.\n",
    "!chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1706711883389,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "ifYrMgH8SK-R",
    "outputId": "a6d5d0ab-93c6-4ab1-b412-edb83d9b6871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      " 20:02:32 up 17 min,  0 users,  load average: 0.17, 0.44, 0.40\n"
     ]
    }
   ],
   "source": [
    "# Conneting with our local machine (essentially, we are just connecting to our own machine \"as if\" it is a remote server.)\n",
    "# pptime will just tell us how long our system has been running.\n",
    "!ssh -o StrictHostKeyChecking=no localhost uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgiqs2DGdDxE"
   },
   "source": [
    "## Section 2 - Installing and Configuring Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1706711883389,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "a_PPILDLUm7Q"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29647,
     "status": "ok",
     "timestamp": 1706711913035,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "PF40OFaEU6C5",
    "outputId": "7adc647d-365e-4678-b374-d270c6a9e18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-14 20:02:32--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 706533213 (674M) [application/x-gzip]\n",
      "Saving to: ‘hadoop-3.3.5.tar.gz’\n",
      "\n",
      "hadoop-3.3.5.tar.gz 100%[===================>] 673.80M  11.0MB/s    in 65s     \n",
      "\n",
      "2024-02-14 20:03:37 (10.4 MB/s) - ‘hadoop-3.3.5.tar.gz’ saved [706533213/706533213]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Installing Hadoop and configuring JAVA_HOME:\n",
    "# Downloading Hadoop\n",
    "# Upzipping\n",
    "# Copying hadoop into our /usr/local folder\n",
    "# Removing the unused original copy\n",
    "# Remove the compressed (zip) file, we're not using it anymore.\n",
    "# Adding a variable called \"JAVA_HOME\" to hadoop's environment script which tells it where Java is on our system.\n",
    "\n",
    "!if [ ! -d /usr/local/hadoop-3.3.5/ ]; then \\\n",
    "wget -4 https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz; \\\n",
    "tar -xzf hadoop-3.3.5.tar.gz; \\\n",
    "cp -r hadoop-3.3.5/ /usr/local/; \\\n",
    "rm -rf hadoop-3.3.5/; \\\n",
    "rm hadoop-3.3.5.tar.gz; \\\n",
    "echo \"export JAVA_HOME=$(dirname $(dirname $(realpath $(which java))))\" >> /usr/local/hadoop-3.3.5/etc/hadoop/hadoop-env.sh; \\\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706711913036,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "Scb-Xsb6VKCR"
   },
   "outputs": [],
   "source": [
    "# Setting up some of our environmental variables:\n",
    "# Here we add Hadoop's location to our path (in Python) and tell our system where hadoop is located.\n",
    "os.environ['PATH'] = \"/usr/local/hadoop-3.3.5/bin/:\" + os.environ['PATH']\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eu-h0GUO-CJ"
   },
   "source": [
    "As mentioned in the intro, we'll need to do a small bit of configuration to get the pseudo-distributed mode running as it should. To do this, we'll need to set some properties in the site XML configuration files. This way we can tell Hadoop which machines are in the cluster and where we want to run the daemons.\n",
    "\n",
    "You can find the specifics of this [here](https://hadoop.apache.org/docs/r3.3.3/hadoop-project-dist/hadoop-common/SingleCluster.html), but I have included a script you can run to add the correct properties to the correct files (`lab3_config.sh`). The files we'll be editing can be found at `$HADOOP_HOME/etc/hadoop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20407,
     "status": "ok",
     "timestamp": 1706711933439,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "PtcTKhF7S5Y5",
    "outputId": "60637c02-ea89-45d4-f168-9f4430c02eec"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1706711934170,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "V0C48lcxTLFs",
    "outputId": "20a32dac-0174-4877-94f3-a619165fcd33"
   },
   "outputs": [],
   "source": [
    "# Here, we've uploaded lab3_config.sh to our Google drive , not that this path will depend on **YOUR** google drive.\n",
    "# We are then copying this to our \"content\" folder.\n",
    "# This does a lot of the annoying configuration for us.\n",
    "# !cp /content/drive/MyDrive/Temp/lab3_config.sh /content/\n",
    "# !cat lab3_config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1706711934488,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "gtviGruZVCh4"
   },
   "outputs": [],
   "source": [
    "# Running our config script\n",
    "# Note: remember to check you have the correct filepath\n",
    "!bash lab3_config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppvjY9e5Q7Jb"
   },
   "source": [
    "Before HDFS can be used, the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3686,
     "status": "ok",
     "timestamp": 1706711938170,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "RP7Oxg88QUqd",
    "outputId": "8f039f78-09ad-4c1a-f343-ec1b6a9d55a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/hadoop-3.3.5/logs does not exist. Creating.\n",
      "2024-02-14 20:03:52,714 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = 56cc797895e3/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 3.3.5\n",
      "STARTUP_MSG:   classpath = /usr/local/hadoop-3.3.5/etc/hadoop:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-all-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-stomp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-rxtx-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-xml-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-sctp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jettison-1.5.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-http-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-dns-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-smtp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-socks-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-proxy-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-xml-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-servlet-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-buffer-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-common-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-haproxy-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-udt-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-webapp-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-mqtt-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-ajax-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http2-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/hadoop-annotations-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-security-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-redis-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-memcache-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/hadoop-auth-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-server-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jetty-io-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/hadoop-nfs-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/hadoop-registry-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/hadoop-kms-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-all-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-xml-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jettison-1.5.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-http-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-dns-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-socks-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-xml-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-servlet-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-buffer-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-common-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-udt-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-webapp-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http2-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-annotations-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-security-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-redis-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-auth-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-server-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-io-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.77.Final.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5-tests.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-server-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/asm-commons-9.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-common-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/asm-tree-9.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-jndi-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-api-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/asm-analysis-9.3.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-client-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-plus-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-servlet-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-client-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/snakeyaml-1.32.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-annotations-9.4.48.v20220622.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-api-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-common-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-registry-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-client-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-core-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-common-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-api-3.3.5.jar:/usr/local/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-router-3.3.5.jar\n",
      "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 706d88266abcee09ed78fbaa0ad5f74d818ab0e9; compiled by 'stevel' on 2023-03-15T15:56Z\n",
      "STARTUP_MSG:   java = 11.0.21\n",
      "************************************************************/\n",
      "2024-02-14 20:03:52,722 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2024-02-14 20:03:52,845 INFO namenode.NameNode: createNameNode [-format]\n",
      "2024-02-14 20:03:52,993 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-02-14 20:03:53,194 INFO namenode.NameNode: Formatting using clusterid: CID-5c5405c1-90cc-4bd2-8214-85b7dc05b09d\n",
      "2024-02-14 20:03:53,216 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2024-02-14 20:03:53,235 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2024-02-14 20:03:53,236 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2024-02-14 20:03:53,236 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2024-02-14 20:03:53,259 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
      "2024-02-14 20:03:53,259 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
      "2024-02-14 20:03:53,259 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
      "2024-02-14 20:03:53,259 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
      "2024-02-14 20:03:53,259 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2024-02-14 20:03:53,330 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2024-02-14 20:03:53,428 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
      "2024-02-14 20:03:53,428 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2024-02-14 20:03:53,430 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2024-02-14 20:03:53,430 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Feb 14 20:03:53\n",
      "2024-02-14 20:03:53,431 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2024-02-14 20:03:53,431 INFO util.GSet: VM type       = 64-bit\n",
      "2024-02-14 20:03:53,432 INFO util.GSet: 2.0% max memory 1.9 GB = 39.3 MB\n",
      "2024-02-14 20:03:53,432 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2024-02-14 20:03:53,468 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2024-02-14 20:03:53,468 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2024-02-14 20:03:53,473 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2024-02-14 20:03:53,504 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2024-02-14 20:03:53,504 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2024-02-14 20:03:53,504 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2024-02-14 20:03:53,504 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2024-02-14 20:03:53,536 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2024-02-14 20:03:53,536 INFO util.GSet: VM type       = 64-bit\n",
      "2024-02-14 20:03:53,536 INFO util.GSet: 1.0% max memory 1.9 GB = 19.6 MB\n",
      "2024-02-14 20:03:53,536 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2024-02-14 20:03:53,541 INFO namenode.FSDirectory: ACLs enabled? true\n",
      "2024-02-14 20:03:53,541 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2024-02-14 20:03:53,541 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2024-02-14 20:03:53,541 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2024-02-14 20:03:53,545 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2024-02-14 20:03:53,546 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2024-02-14 20:03:53,550 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2024-02-14 20:03:53,551 INFO util.GSet: VM type       = 64-bit\n",
      "2024-02-14 20:03:53,551 INFO util.GSet: 0.25% max memory 1.9 GB = 4.9 MB\n",
      "2024-02-14 20:03:53,551 INFO util.GSet: capacity      = 2^19 = 524288 entries\n",
      "2024-02-14 20:03:53,564 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2024-02-14 20:03:53,564 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2024-02-14 20:03:53,565 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2024-02-14 20:03:53,569 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2024-02-14 20:03:53,569 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2024-02-14 20:03:53,570 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2024-02-14 20:03:53,570 INFO util.GSet: VM type       = 64-bit\n",
      "2024-02-14 20:03:53,570 INFO util.GSet: 0.029999999329447746% max memory 1.9 GB = 603.3 KB\n",
      "2024-02-14 20:03:53,570 INFO util.GSet: capacity      = 2^16 = 65536 entries\n",
      "2024-02-14 20:03:53,584 INFO namenode.FSImage: Allocated new BlockPoolId: BP-583198845-172.17.0.2-1707941033579\n",
      "2024-02-14 20:03:53,594 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
      "2024-02-14 20:03:53,620 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2024-02-14 20:03:53,706 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
      "2024-02-14 20:03:53,716 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2024-02-14 20:03:53,737 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2024-02-14 20:03:53,738 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2024-02-14 20:03:53,740 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2024-02-14 20:03:53,741 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at 56cc797895e3/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706711938171,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "0XqLvC7BRNMt"
   },
   "outputs": [],
   "source": [
    "# Creating our HDFS environment variables:\n",
    "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
    "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
    "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
    "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
    "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZxnrLJhRKnV"
   },
   "source": [
    "Hadoop comes with scripts for running commands, and starting and stopping daemons across the whole cluster. These scripts can be found in the `bin` and `sbin` directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23307,
     "status": "ok",
     "timestamp": 1706711961475,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "0Ydm_FwbRQHd",
    "outputId": "5ad6ab89-3beb-462a-d4be-1eebbbb8e58c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [56cc797895e3]\n",
      "56cc797895e3: Warning: Permanently added '56cc797895e3' (ED25519) to the list of known hosts.\n",
      "2024-02-14 20:04:02,912 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Launching hdfs daemons\n",
    "!$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1706711961852,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "8XqjhHSBRSF1",
    "outputId": "1dd43d18-9d28-4ce0-847c-825d2f81d0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 DataNode\n",
      "1196 SecondaryNameNode\n",
      "1324 Jps\n",
      "861 NameNode\n"
     ]
    }
   ],
   "source": [
    "# Listing the running daemons\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12803,
     "status": "ok",
     "timestamp": 1706711974653,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "GvsrIN1ASZZM",
    "outputId": "e6e0b4e5-2612-48ba-cd1f-835b9ee0497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Launching our yarn daemons\n",
    "# nohup causes a process to ignore a \"hang-up\" signal\n",
    "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1706711975995,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "E1wYsh04SeKN",
    "outputId": "808f0bbc-6ce4-4b87-a6eb-4f9c6b99d634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008 DataNode\n",
      "1557 NodeManager\n",
      "1929 Jps\n",
      "1436 ResourceManager\n",
      "1196 SecondaryNameNode\n",
      "861 NameNode\n"
     ]
    }
   ],
   "source": [
    "# Listing the running daemons (i.e. programs doing some work in the background for us\n",
    "# Note the NodeManager, and Resource Manager\n",
    "# More info on JPS here: https://www.oreilly.com/library/view/apache-hadoop-3/9781788999830/81815d09-e275-44e2-88a8-4f63ab943b92.xhtml\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7541,
     "status": "ok",
     "timestamp": 1706711983534,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "M97p1FpHShvP",
    "outputId": "aaaa673b-7776-4445-8c10-34e53397e4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:09,882 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Configured Capacity: 62671097856 (58.37 GB)\n",
      "Present Capacity: 17729007616 (16.51 GB)\n",
      "DFS Remaining: 17728983040 (16.51 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 56cc797895e3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62671097856 (58.37 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 41725374464 (38.86 GB)\n",
      "DFS Remaining: 17728983040 (16.51 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 28.29%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Feb 14 20:04:07 GMT 2024\n",
      "Last Block Report: Wed Feb 14 20:03:58 GMT 2024\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the basic file system information and statistics to make sure everything is set up as it should be:\n",
    "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkQmp7IgnVBn"
   },
   "source": [
    "## Section 3 - Running a program\n",
    "\n",
    "The default installation comes with several MapReduce examples already installed. We can see below that this includes a basic wordcount program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2993,
     "status": "ok",
     "timestamp": 1706711986524,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "HiXv21xlsD1M",
    "outputId": "23213491-3502-4dcd-dfc9-b71e249e93cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example program must be given as the first argument.\n",
      "Valid program names are:\n",
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
      "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
      "  dbcount: An example job that count the pageview counts from a database.\n",
      "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
      "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
      "  join: A job that effects a join over sorted, equally partitioned datasets\n",
      "  multifilewc: A job that counts words from several files.\n",
      "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
      "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
      "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
      "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
      "  secondarysort: An example defining a secondary sort to the reduce.\n",
      "  sort: A map/reduce program that sorts the data written by the random writer.\n",
      "  sudoku: A sudoku solver.\n",
      "  teragen: Generate data for the terasort\n",
      "  terasort: Run the terasort\n",
      "  teravalidate: Checking results of terasort\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVtMdbx7sKsh"
   },
   "source": [
    "We'll use this sample program to start. First lets make some input for our program \"locally\" before adding it to the HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1706711986853,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "ZKabzsdtnyLC"
   },
   "outputs": [],
   "source": [
    "!mkdir input\n",
    "!echo \"Hello world from COMP30770\" > input/f1.txt\n",
    "!echo \"Hello there from UCD\" > input/f2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 11474,
     "status": "ok",
     "timestamp": 1706711998325,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "CNwMheinoKcY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:11,964 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-02-14 20:04:12,980 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-02-14 20:04:14,683 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# First we'll need to make the corresponding input dir on our HDFS.\n",
    "# Don't panic!! Let's break this down.\n",
    "# 1. We run \"hdfs\"\n",
    "# 2. We pass the argument \"dfs\", this tell Hadppot that we are going to use a file system command.\n",
    "# 3. We then make a directory in on our HDFS.\n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
    "\n",
    "# We can add files to the HDFS in two ways...\n",
    "# 1. Using \"copyFromLocal\":\n",
    "!$HADOOP_HOME/bin/hdfs dfs -copyFromLocal input/f1.txt /word_count\n",
    "\n",
    "# 2. Using \"put\":\n",
    "!$HADOOP_HOME/bin/hdfs dfs -put input/f2.txt /word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3842,
     "status": "ok",
     "timestamp": 1706712002153,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "pBzk0_5rpzcu",
    "outputId": "37c4f939-8954-4886-bf0a-a1a8a6190c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:15,774 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup         27 2024-02-14 20:04 /word_count/f1.txt\n",
      "-rw-r--r--   1 root supergroup         21 2024-02-14 20:04 /word_count/f2.txt\n"
     ]
    }
   ],
   "source": [
    "# We can see our files now on the HDFS:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39934,
     "status": "ok",
     "timestamp": 1706712042085,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "KliYqvljqKaA",
    "outputId": "bcb9efbb-ab18-4c9a-8297-15650c5034b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:16,849 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-02-14 20:04:17,293 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-02-14 20:04:17,550 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707941046409_0001\n",
      "2024-02-14 20:04:17,720 INFO input.FileInputFormat: Total input files to process : 2\n",
      "2024-02-14 20:04:18,181 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-02-14 20:04:18,331 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707941046409_0001\n",
      "2024-02-14 20:04:18,331 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-14 20:04:18,459 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-14 20:04:18,459 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-14 20:04:18,619 INFO impl.YarnClientImpl: Submitted application application_1707941046409_0001\n",
      "2024-02-14 20:04:18,642 INFO mapreduce.Job: The url to track the job: http://56cc797895e3:8088/proxy/application_1707941046409_0001/\n",
      "2024-02-14 20:04:18,642 INFO mapreduce.Job: Running job: job_1707941046409_0001\n",
      "2024-02-14 20:04:25,842 INFO mapreduce.Job: Job job_1707941046409_0001 running in uber mode : false\n",
      "2024-02-14 20:04:25,847 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-14 20:04:31,943 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-02-14 20:04:35,965 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-14 20:04:35,979 INFO mapreduce.Job: Job job_1707941046409_0001 completed successfully\n",
      "2024-02-14 20:04:36,076 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=102\n",
      "\t\tFILE: Number of bytes written=828215\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=256\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6385\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1567\n",
      "\t\tTotal time spent by all map tasks (ms)=6385\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1567\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6385\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1567\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6538240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1604608\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=108\n",
      "\t\tInput split bytes=208\n",
      "\t\tCombine input records=8\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=108\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tCPU time spent (ms)=1660\n",
      "\t\tPhysical memory (bytes) snapshot=832020480\n",
      "\t\tVirtual memory (bytes) snapshot=8244445184\n",
      "\t\tTotal committed heap usage (bytes)=585105408\n",
      "\t\tPeak Map Physical memory (bytes)=315871232\n",
      "\t\tPeak Map Virtual memory (bytes)=2743545856\n",
      "\t\tPeak Reduce Physical memory (bytes)=210579456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2758066176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=48\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n"
     ]
    }
   ],
   "source": [
    "# Executing our first mapreduce wordcount program\n",
    "# We are using the \"wordcount\" program here with the files we created as the argument.\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar wordcount /word_count/*.txt /word_count/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3773,
     "status": "ok",
     "timestamp": 1706712045845,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "H7UdsyPZqKXp",
    "outputId": "a61e3e9a-2c46-4101-9802-6c8cfc51628f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:36,744 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2024-02-14 20:04 /word_count/output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         49 2024-02-14 20:04 /word_count/output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1706712050202,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "E2Yb5BVqqKVT",
    "outputId": "4e79e4bf-0a84-4086-cb51-8329228ec32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:04:37,817 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "COMP30770\t1\n",
      "Hello\t2\n",
      "UCD\t1\n",
      "from\t2\n",
      "there\t1\n",
      "world\t1\n"
     ]
    }
   ],
   "source": [
    "# part-r-00000 contains the actual ouput:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPAkfcaNrWw6"
   },
   "source": [
    "We can also make use of Hadoop's streaming feature to write a MapReduce programs in other languages, like Python! The feature will create the job for us, submit that job to the appropriate cluster and monitor it's progress until finished.\n",
    "\n",
    "We'll need to write a mapper and reducer scripts in Python as below. Both of these scripts read their input from STDIN (line by line) and send output via STDOUT so we'll make use of the `sys` module here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1706712050202,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "RuK_tT4XtcWJ",
    "outputId": "9f510623-1e57-4ef4-eced-972bf4a3177b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "\n",
    "  for word in words:\n",
    "    print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1706712050202,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "a66f4O6svDx7",
    "outputId": "afcdda85-cba1-47cd-801c-c7f2abb57e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "\n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "\n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: # to avoid None values\n",
    "      print('%s\\t%s' % (current_word, current_count))\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "  print('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/lab3\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1706712050202,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "ZG4i6nuLzo4e"
   },
   "outputs": [],
   "source": [
    "# Giving these new files permissions:\n",
    "!chmod u+x /workdir/lab3/mapper.py /workdir/lab3/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41127,
     "status": "ok",
     "timestamp": 1706712091317,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "Fs9cX0O8zzGW",
    "outputId": "a9fb970c-baf3-413f-eabf-0aff669335db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:05:14,863 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /word_count/python_output\n",
      "2024-02-14 20:05:17,015 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar4563505953634249597/] [] /tmp/streamjob9671179864342969534.jar tmpDir=null\n",
      "2024-02-14 20:05:17,540 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-02-14 20:05:17,641 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-02-14 20:05:17,768 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707941046409_0003\n",
      "2024-02-14 20:05:17,955 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2024-02-14 20:05:17,994 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2024-02-14 20:05:18,747 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1707941046409_0003\n",
      "2024-02-14 20:05:18,747 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-02-14 20:05:18,988 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-02-14 20:05:18,989 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-02-14 20:05:19,171 INFO impl.YarnClientImpl: Submitted application application_1707941046409_0003\n",
      "2024-02-14 20:05:19,243 INFO mapreduce.Job: The url to track the job: http://56cc797895e3:8088/proxy/application_1707941046409_0003/\n",
      "2024-02-14 20:05:19,245 INFO mapreduce.Job: Running job: job_1707941046409_0003\n",
      "2024-02-14 20:05:25,387 INFO mapreduce.Job: Job job_1707941046409_0003 running in uber mode : false\n",
      "2024-02-14 20:05:25,390 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-02-14 20:05:32,536 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-02-14 20:05:38,201 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-02-14 20:05:39,223 INFO mapreduce.Job: Job job_1707941046409_0003 completed successfully\n",
      "2024-02-14 20:05:39,362 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=86\n",
      "\t\tFILE: Number of bytes written=1112661\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14565\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3620\n",
      "\t\tTotal time spent by all map tasks (ms)=14565\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3620\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14565\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3620\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14914560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3706880\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=64\n",
      "\t\tMap output materialized bytes=98\n",
      "\t\tInput split bytes=273\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=98\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=94\n",
      "\t\tCPU time spent (ms)=2860\n",
      "\t\tPhysical memory (bytes) snapshot=1004634112\n",
      "\t\tVirtual memory (bytes) snapshot=10980417536\n",
      "\t\tTotal committed heap usage (bytes)=895483904\n",
      "\t\tPeak Map Physical memory (bytes)=296960000\n",
      "\t\tPeak Map Virtual memory (bytes)=2748080128\n",
      "\t\tPeak Reduce Physical memory (bytes)=190918656\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2746490880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=51\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n",
      "2024-02-14 20:05:39,363 INFO streaming.StreamJob: Output directory: /word_count/python_output\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "\n",
    "!$HADOOP_HOME/bin/hdfs dfs -rm -r /word_count/python_output\n",
    "\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar \\\n",
    "  -input /word_count/*.txt \\\n",
    "  -output /word_count/python_output \\\n",
    "  -mapper \"python3 /workdir/lab3/mapper.py\" \\\n",
    "  -reducer \"python3 /workdir/lab3/reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3360,
     "status": "ok",
     "timestamp": 1706712094674,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "G6pLpDvj0PEn",
    "outputId": "b864b294-a60a-4217-d80f-2a85521463ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:05:57,545 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2024-02-14 20:05 /word_count/python_output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         49 2024-02-14 20:05 /word_count/python_output/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new python_output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/python_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3137,
     "status": "ok",
     "timestamp": 1706712097804,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "dqxwcP2B1Roz",
    "outputId": "1ce3f6cf-32b4-4838-cc51-8aab1e996cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 20:05:58,992 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "COMP30770\t1\n",
      "Hello\t2\n",
      "UCD\t1\n",
      "from\t2\n",
      "there\t1\n",
      "world\t1\n"
     ]
    }
   ],
   "source": [
    "# part-00000 contains the ouput this time:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/python_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPHIG4N_s8tP"
   },
   "source": [
    "## Section 4 - Modifying the WordCount program in Java\n",
    "\n",
    "The exercises in Section 5 can be completed in either Java or Python - whichever you prefer. We want our final output to perform the same task as above - the aim here is to become comfortable making small changes to the WordCount example.\n",
    "\n",
    "If you are completing this task in Java, you will need to recompile and repackage the code after editing it, or you will run the un-edited version of the code. We'll run through an example below before coming to the exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIQKtRcx2_tX"
   },
   "source": [
    "**Java Example:**\n",
    "\n",
    "First download the Java source code of the WordCount example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1961,
     "status": "ok",
     "timestamp": 1706712099754,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "BE_zixR2qKSb",
    "outputId": "8eb3f339-1d2c-41c7-dd86-0e53ef8b79b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-14 20:06:20--  http://csserver.ucd.ie/~aventresque/COMP30770/2020/WordCount.java\n",
      "Resolving csserver.ucd.ie (csserver.ucd.ie)... 193.1.133.60\n",
      "Connecting to csserver.ucd.ie (csserver.ucd.ie)|193.1.133.60|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://csserver.ucd.ie/~aventresque/COMP30770/2020/WordCount.java [following]\n",
      "--2024-02-14 20:06:20--  https://csserver.ucd.ie/~aventresque/COMP30770/2020/WordCount.java\n",
      "Connecting to csserver.ucd.ie (csserver.ucd.ie)|193.1.133.60|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2089 (2.0K) [text/x-java]\n",
      "Saving to: ‘WordCount.java’\n",
      "\n",
      "WordCount.java      100%[===================>]   2.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-02-14 20:06:21 (67.8 MB/s) - ‘WordCount.java’ saved [2089/2089]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate csserver.ucd.ie/~aventresque/COMP30770/2020/WordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r3_0p724eXf"
   },
   "source": [
    "Now set Hadoop’s classpath and compile the file (NB - the following steps are quite precise, so make sure you do them all in order each time you want to edit the WordCount file!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 3935,
     "status": "ok",
     "timestamp": 1706712103687,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "HiCzs05h4VgF"
   },
   "outputs": [],
   "source": [
    "# Setting Hadoop's classpath:\n",
    "!export HADOOP_CLASSPATH=/usr/lib/jvm/java-1.8.0-openjdk-amd64/lib/tools.jar\n",
    "\n",
    "# Recompiling:\n",
    "!$HADOOP_HOME/bin/hadoop com.sun.tools.javac.Main WordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvB2OY3cWHZp"
   },
   "source": [
    "**Note:** You can ignore any bad substitution warnings you might receive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1706712104035,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "MiUTDFmG4r3L"
   },
   "outputs": [],
   "source": [
    "# Repackaging:\n",
    "!jar cf wc.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34866,
     "status": "ok",
     "timestamp": 1706712138899,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "HbeKRrnw4wd8",
    "outputId": "881bfb75-a058-4739-f4bd-546de22d7cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-31 14:41:47,204 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-01-31 14:41:47,718 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2024-01-31 14:41:47,762 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1706711979035_0003\n",
      "2024-01-31 14:41:48,130 INFO input.FileInputFormat: Total input files to process : 2\n",
      "2024-01-31 14:41:48,639 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-01-31 14:41:49,009 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1706711979035_0003\n",
      "2024-01-31 14:41:49,010 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-01-31 14:41:49,254 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-01-31 14:41:49,256 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-01-31 14:41:49,353 INFO impl.YarnClientImpl: Submitted application application_1706711979035_0003\n",
      "2024-01-31 14:41:49,399 INFO mapreduce.Job: The url to track the job: http://ea1ec48c77dd:8088/proxy/application_1706711979035_0003/\n",
      "2024-01-31 14:41:49,400 INFO mapreduce.Job: Running job: job_1706711979035_0003\n",
      "2024-01-31 14:41:58,719 INFO mapreduce.Job: Job job_1706711979035_0003 running in uber mode : false\n",
      "2024-01-31 14:41:58,720 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-01-31 14:42:08,922 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-01-31 14:42:18,028 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-01-31 14:42:18,041 INFO mapreduce.Job: Job job_1706711979035_0003 completed successfully\n",
      "2024-01-31 14:42:18,183 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=102\n",
      "\t\tFILE: Number of bytes written=827459\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=256\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16253\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6183\n",
      "\t\tTotal time spent by all map tasks (ms)=16253\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6183\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16253\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6183\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16643072\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6331392\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=80\n",
      "\t\tMap output materialized bytes=108\n",
      "\t\tInput split bytes=208\n",
      "\t\tCombine input records=8\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=108\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=217\n",
      "\t\tCPU time spent (ms)=2550\n",
      "\t\tPhysical memory (bytes) snapshot=813031424\n",
      "\t\tVirtual memory (bytes) snapshot=8177459200\n",
      "\t\tTotal committed heap usage (bytes)=762314752\n",
      "\t\tPeak Map Physical memory (bytes)=305516544\n",
      "\t\tPeak Map Virtual memory (bytes)=2724564992\n",
      "\t\tPeak Reduce Physical memory (bytes)=211451904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2732933120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=48\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n"
     ]
    }
   ],
   "source": [
    "# Running our \"new\" WordCount program on our basic example:\n",
    "!$HADOOP_HOME/bin/hadoop jar wc.jar WordCount /word_count/*.txt /word_count/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2991,
     "status": "ok",
     "timestamp": 1706712141877,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "O9sc2dQv5cOD",
    "outputId": "7d99252e-06f1-47b4-d63a-86082c57fb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2024-01-31 14:42 /word_count/output2/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         49 2024-01-31 14:42 /word_count/output2/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3490,
     "status": "ok",
     "timestamp": 1706712145355,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "OPeufkuD51Lw",
    "outputId": "fdc83fd6-cba1-4a66-b4e3-77a23a1e8f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP30770\t1\n",
      "Hello\t2\n",
      "UCD\t1\n",
      "from\t2\n",
      "there\t1\n",
      "world\t1\n"
     ]
    }
   ],
   "source": [
    "# Again, part-r-00000 contains the actual ouput:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output2/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lggA6Tzginxf"
   },
   "source": [
    "## Section 5 - Exercises\n",
    "\n",
    "First, download the folliwing 5 books using `wget`:\n",
    "\n",
    "* http://www.gutenberg.org/files/1524/1524-0.txt\n",
    "* http://www.gutenberg.org/files/1112/1112-0.txt\n",
    "* http://www.gutenberg.org/files/2267/2267.txt\n",
    "* http://www.gutenberg.org/files/1513/1513-0.txt\n",
    "* http://www.gutenberg.org/files/1121/1121-0.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1706712145355,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "hfzkwp1xiVR8"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od_tYg58jHGc"
   },
   "source": [
    "Upload the 6 books in your HDFS. Modify the WordCount example (in either language) to answer some of the following questions. They will require you to change just a few changes, or even no changes.\n",
    "\n",
    "Note: do not forget to recompile (hadoop com.sun.tools.javac.Main ClassName.java) and repackage (jar cf wc.jar ClassName*.class) the Java code after editing it, or you will run the un-edited version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1706712145356,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "b3a_8_FGjAn7"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4qYJOgClAPS"
   },
   "source": [
    "1. Run the WordCount example. How many unique words does the corpus contain?\n",
    "Hint: the mapreduce program gives us a lot of outputs - is there a line that tells us how many records were handed to the reduce function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLMcoeK8qHaQ"
   },
   "source": [
    "2. Make some changes to the map function to reduce the number of unique terms found (e.g. remove non-alphanumeric terms, make strings lowercase) and run the job again. Which changes did you make and how do they influence the number of unique terms found?\n",
    "\n",
    "  **Java Hint:** Look through your code to find `word.set(itr.nextToken());` - we want to make some changes here in the format of `word.set(itr.nextToken().DOSOMETHING!);`. You can find some help with this [here](https://www.w3schools.com/java/java_ref_string.asp).\n",
    "\n",
    "  **Python Hint:** You could take a look at the Python RegEx module [here](https://www.w3schools.com/python/python_regex.asp). You might also want to take a look at some string methods [here](https://www.w3schools.com/python/python_ref_string.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1706712145356,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "uKFmoLmaRtMk",
    "outputId": "ef085b13-841f-4e2c-bd7b-44ab1462711b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "\n",
    "  for word in words:\n",
    "\n",
    "    word = re.sub('[^0-9a-zA-Z]', '', word.lower())\n",
    "\n",
    "    if word == '':\n",
    "      continue\n",
    "    else:\n",
    "      print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706712145356,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "MlUC4JGwRxqP",
    "outputId": "2a2a9f52-5f0d-4c7e-f775-f18a5a8f38b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "\n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "\n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: # to avoid None values\n",
    "      print('%s\\t%s' % (current_word, current_count))\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "  print('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1706712145664,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "wKmeA6AfShaf"
   },
   "outputs": [],
   "source": [
    "!chmod u+x /content/mapper.py /content/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6128,
     "status": "ok",
     "timestamp": 1706712151791,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "qqa0G00_SK67",
    "outputId": "830e41a3-e70b-4efe-af3e-7187befe39bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar6415954454821354174/] [] /tmp/streamjob1516436789155777311.jar tmpDir=null\n",
      "2024-01-31 14:42:30,056 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-01-31 14:42:30,296 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2024-01-31 14:42:30,742 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1706711979035_0004\n",
      "2024-01-31 14:42:31,144 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1706711979035_0004\n",
      "2024-01-31 14:42:31,157 ERROR streaming.StreamJob: Error Launching job : Input Pattern hdfs://localhost:9000/exercise_folder/*.txt matches 0 files\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar \\\n",
    "  -input /exercise_folder/*.txt \\\n",
    "  -output /exercise_folder/python_output \\\n",
    "  -mapper \"python /content/mapper.py\" \\\n",
    "  -reducer \"python /content/reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3014,
     "status": "ok",
     "timestamp": 1706712154793,
     "user": {
      "displayName": "Haotian Li",
      "userId": "09451053382319173913"
     },
     "user_tz": -480
    },
    "id": "jejq2_L9ZKRe",
    "outputId": "2396f76c-f857-4772-fc01-9a9349f29b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/exercise_folder/python_output/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -cat /exercise_folder/python_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2osMpwFUr1mU"
   },
   "source": [
    "3. How many words appear less than 4 times?\n",
    "\n",
    "  **Hint:** You can either perform this check manually (by checking outputs) or if you’re feeling adventurous, you could experiment with adding some code to the reducer!\n",
    "  \n",
    "  **Java Hint:** Your results will not be an int but an \"IntWritable\" - we can use `.get()` to grab a value that we can compare!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBMfgr-hs6El"
   },
   "source": [
    "4. What are the most frequent words that end in -ing?\n",
    "\n",
    "  **Hint:** Check out those string methods again!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
